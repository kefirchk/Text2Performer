{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e5e6b98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T14:54:06.882166900Z",
     "start_time": "2025-04-06T14:53:51.864524700Z"
    }
   },
   "outputs": [],
   "source": [
    "from models import create_model\n",
    "from utils.options import parse\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13cf979d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T14:54:06.998372600Z",
     "start_time": "2025-04-06T14:54:06.885610700Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd10e1fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T14:54:16.226067800Z",
     "start_time": "2025-04-06T14:54:07.003838200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 32, 16) = 131072 dimensions.\n"
     ]
    }
   ],
   "source": [
    "app_opt_path = './configs/sampler/sampler_high_res.yml'\n",
    "app_opt = parse(app_opt_path, is_train=False)\n",
    "app_opt['pretrained_sampler'] = './pretrained_models/sampler_high_res.pth'\n",
    "app_opt['dist'] = False\n",
    "app_model = create_model(app_opt)\n",
    "app_model.load_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8d0734a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T14:54:40.746890900Z",
     "start_time": "2025-04-06T14:54:31.374546900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 32, 16) = 131072 dimensions.\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\DIPLOMA\\Code\\Text2Performer\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\DIPLOMA\\Code\\Text2Performer\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: D:\\DIPLOMA\\Code\\Text2Performer\\venv\\Lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n"
     ]
    }
   ],
   "source": [
    "motion_opt_path = './configs/video_transformer/video_trans_high_res.yml'\n",
    "motion_opt = parse(motion_opt_path, is_train=False)\n",
    "motion_opt['pretrained_sampler'] = './pretrained_models/video_trans_high_res.pth'\n",
    "motion_opt['dist'] = False\n",
    "motion_model = create_model(motion_opt)\n",
    "motion_model.load_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36138881",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T14:54:40.825400500Z",
     "start_time": "2025-04-06T14:54:40.763178400Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_raw_image(img_path, downsample=True):\n",
    "    with open(img_path, 'rb') as f:\n",
    "        image = Image.open(f)\n",
    "        width, height = image.size\n",
    "        if downsample:\n",
    "            width = width // 2\n",
    "            height = height // 2\n",
    "        image = image.resize(\n",
    "            size=(width, height), resample=Image.LANCZOS)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a0af0b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T14:54:40.825814800Z",
     "start_time": "2025-04-06T14:54:40.808612800Z"
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a79fb3ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T14:54:40.884821700Z",
     "start_time": "2025-04-06T14:54:40.825814800Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.util import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9460c5e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T14:54:40.953977700Z",
     "start_time": "2025-04-06T14:54:40.885324Z"
    }
   },
   "outputs": [],
   "source": [
    "set_random_seed(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9997a03a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T14:54:40.964503600Z",
     "start_time": "2025-04-06T14:54:40.959428100Z"
    }
   },
   "outputs": [],
   "source": [
    "save_dir = './results'\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19d8027d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T14:55:37.003907700Z",
     "start_time": "2025-04-06T14:54:42.754015200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample timestep    1\r"
     ]
    }
   ],
   "source": [
    "x_identity, x_pose = app_model.sample_appearance(\n",
    "    ['\"The dress the person wears has long sleeves and it is of short length. Its texture is pure color.'], f'{save_dir}/exampler.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98007fb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T14:58:06.300629Z",
     "start_time": "2025-04-06T14:55:38.830176700Z"
    }
   },
   "outputs": [],
   "source": [
    "video_embeddings_pred = torch.zeros([1, 8*32, 128]).to(motion_model.device)\n",
    "motion_model.sample_multinomial_text_embeddings(x_identity, x_pose, \n",
    "                                                ['The lady moves to the right.'],\n",
    "                                                8, list(range(0, 8)), \n",
    "                                                video_embeddings_pred, \n",
    "                                                f'{save_dir}/sequence1')\n",
    "motion_model.refine_synthesized(x_identity, f'{save_dir}/sequence1')\n",
    "\n",
    "\n",
    "video_embeddings_pred = torch.zeros([1, 8*32, 128]).to(motion_model.device)\n",
    "motion_model.sample_multinomial_text_embeddings(x_identity, x_pose, \n",
    "                                                ['The person is moving to the center from the right.'], \n",
    "                                                8, list(range(0, 8)), \n",
    "                                                video_embeddings_pred, \n",
    "                                                f'{save_dir}/sequence2')\n",
    "motion_model.refine_synthesized(x_identity, f'{save_dir}/sequence2')\n",
    "\n",
    "# video_embeddings_pred = torch.zeros([1, 8*32, 128]).to(motion_model.device)\n",
    "# motion_model.sample_multinomial_text_embeddings(x_identity, x_pose, \n",
    "#                                                 ['She turns right from the front to the side.'], \n",
    "#                                                 8, list(range(0, 8)), \n",
    "#                                                 video_embeddings_pred, \n",
    "#                                                 f'{save_dir}/sequence3')\n",
    "# motion_model.refine_synthesized(x_identity, f'{save_dir}/sequence3')\n",
    "# \n",
    "# video_embeddings_pred = torch.zeros([1, 8*32, 128]).to(motion_model.device)\n",
    "# motion_model.sample_multinomial_text_embeddings(x_identity, x_pose, \n",
    "#                                                 ['She turns right from the side to the back.'], \n",
    "#                                                 8, list(range(0, 8)), \n",
    "#                                                 video_embeddings_pred, \n",
    "#                                                 f'{save_dir}/sequence4')\n",
    "# motion_model.refine_synthesized(x_identity, f'{save_dir}/sequence4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ac65985",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T14:58:24.570787600Z",
     "start_time": "2025-04-06T14:58:24.531073900Z"
    }
   },
   "outputs": [],
   "source": [
    "def inter_sequence_inter(first_seq_idx, second_seq_idx):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    video_embeddings_pred = torch.zeros([1, 8*32, 128]).to(motion_model.device)\n",
    "\n",
    "    first_frame_path = f'{save_dir}/sequence{first_seq_idx}/007.png'\n",
    "    first_frame = load_raw_image(first_frame_path, downsample=False)\n",
    "    first_frame = np.array(first_frame).transpose(2, 0, 1).astype(np.float32)\n",
    "    first_frame = first_frame / 127.5 - 1\n",
    "    first_frame = torch.from_numpy(first_frame).unsqueeze(0).to(device)\n",
    "\n",
    "    first_frame_embedding = motion_model.get_quantized_frame_embedding(first_frame).view(1, motion_model.img_embed_dim // 2, -1).permute(0, 2, 1).contiguous()\n",
    "\n",
    "    video_embeddings_pred[:, :32, :] = first_frame_embedding\n",
    "\n",
    "    end_frame_path = f'{save_dir}/sequence{second_seq_idx}/000.png'\n",
    "    end_frame = load_raw_image(end_frame_path, downsample=False)\n",
    "    end_frame = np.array(end_frame).transpose(2, 0, 1).astype(np.float32)\n",
    "    end_frame = end_frame / 127.5 - 1\n",
    "    end_frame = torch.from_numpy(end_frame).unsqueeze(0).to(device)\n",
    "\n",
    "    end_frame_embedding = motion_model.get_quantized_frame_embedding(end_frame).view(1, motion_model.img_embed_dim // 2, -1).permute(0, 2, 1).contiguous()\n",
    "\n",
    "    video_embeddings_pred[:, -32:, :] = end_frame_embedding\n",
    "\n",
    "    motion_model.sample_multinomial_text_embeddings(x_identity, x_pose, \n",
    "                                                    ['empty'], \n",
    "                                                    8, list(range(1, 7)), \n",
    "                                                    video_embeddings_pred, \n",
    "                                                    f'{save_dir}/sequence{first_seq_idx}_{second_seq_idx}')\n",
    "    motion_model.refine_synthesized(x_identity, f'{save_dir}/sequence{first_seq_idx}_{second_seq_idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82b4718f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T14:59:36.630887800Z",
     "start_time": "2025-04-06T14:58:26.633420800Z"
    }
   },
   "outputs": [],
   "source": [
    "inter_sequence_inter(1, 2)\n",
    "# inter_sequence_inter(2, 3)\n",
    "# inter_sequence_inter(3, 4)\n",
    "# inter_sequence_inter(4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0df563a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T14:59:56.763073900Z",
     "start_time": "2025-04-06T14:59:56.714038Z"
    }
   },
   "outputs": [],
   "source": [
    "def intra_sequence_inter(seq_idx):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    video_embeddings_pred = torch.zeros([1, 8*32, 128]).to(motion_model.device)\n",
    "    \n",
    "    for frame_idx in range(7):\n",
    "        first_frame_path = f'{save_dir}/sequence{seq_idx}/{frame_idx:03d}.png'\n",
    "        first_frame = load_raw_image(first_frame_path, downsample=False)\n",
    "        first_frame = np.array(first_frame).transpose(2, 0, 1).astype(np.float32)\n",
    "        first_frame = first_frame / 127.5 - 1\n",
    "        first_frame = torch.from_numpy(first_frame).unsqueeze(0).to(device)\n",
    "\n",
    "        first_frame_embedding = motion_model.get_quantized_frame_embedding(first_frame).view(1, motion_model.img_embed_dim // 2, -1).permute(0, 2, 1).contiguous()\n",
    "\n",
    "        video_embeddings_pred[:, :32, :] = first_frame_embedding\n",
    "\n",
    "        end_frame_path = f'{save_dir}/sequence{seq_idx}/{frame_idx+1:03d}.png'\n",
    "        end_frame = load_raw_image(end_frame_path, downsample=False)\n",
    "        end_frame = np.array(end_frame).transpose(2, 0, 1).astype(np.float32)\n",
    "        end_frame = end_frame / 127.5 - 1\n",
    "        end_frame = torch.from_numpy(end_frame).unsqueeze(0).to(device)\n",
    "\n",
    "        end_frame_embedding = motion_model.get_quantized_frame_embedding(end_frame).view(1, motion_model.img_embed_dim // 2, -1).permute(0, 2, 1).contiguous()\n",
    "\n",
    "        video_embeddings_pred[:, -32:, :] = end_frame_embedding\n",
    "\n",
    "        motion_model.sample_multinomial_text_embeddings(x_identity, x_pose, \n",
    "                                                        ['empty'], \n",
    "                                                        8, list(range(1, 7)), \n",
    "                                                        video_embeddings_pred, \n",
    "                                                        f'{save_dir}/sequence{seq_idx}_interpolated',\n",
    "                                                        save_idx=list(range(frame_idx*8, (frame_idx+1)*8)))\n",
    "    \n",
    "    motion_model.refine_synthesized(x_identity, f'{save_dir}/sequence{seq_idx}_interpolated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83633aba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:05:26.044174300Z",
     "start_time": "2025-04-06T15:00:05.195671200Z"
    }
   },
   "outputs": [],
   "source": [
    "intra_sequence_inter(1)\n",
    "# intra_sequence_inter(2)\n",
    "# intra_sequence_inter(3)\n",
    "# intra_sequence_inter(4)\n",
    "# intra_sequence_inter(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72ec7469",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:12:29.181489200Z",
     "start_time": "2025-04-06T15:05:32.006800400Z"
    }
   },
   "outputs": [],
   "source": [
    "intra_sequence_inter('1_2')\n",
    "# intra_sequence_inter('2_3')\n",
    "# intra_sequence_inter('3_4')\n",
    "# intra_sequence_inter('4_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dedc856f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:12:33.293396900Z",
     "start_time": "2025-04-06T15:12:33.237047200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results/sequence1_interpolated\n",
      "./results/sequence1_2_interpolated\n",
      "./results/sequence2\n"
     ]
    }
   ],
   "source": [
    "video_file_name = f'{save_dir}/video.mp4'\n",
    "images = []\n",
    "for seq_idx in range(1, 7):    \n",
    "    if os.path.exists(f'{save_dir}/sequence{seq_idx}_interpolated'):\n",
    "        print(f'{save_dir}/sequence{seq_idx}_interpolated')\n",
    "        for frame_idx in range(56):   \n",
    "            images.append(f'{save_dir}/sequence{seq_idx}_interpolated/{frame_idx:03d}.png')\n",
    "    elif os.path.exists(f'{save_dir}/sequence{seq_idx}'):\n",
    "        print(f'{save_dir}/sequence{seq_idx}')\n",
    "        for frame_idx in range(8):  \n",
    "            images.append(f'{save_dir}/sequence{seq_idx}/{frame_idx:03d}.png')\n",
    "    if os.path.exists(f'{save_dir}/sequence{seq_idx}_{seq_idx+1}_interpolated'):\n",
    "        print(f'{save_dir}/sequence{seq_idx}_{seq_idx+1}_interpolated')\n",
    "        for frame_idx in range(56):\n",
    "            images.append(f'{save_dir}/sequence{seq_idx}_{seq_idx+1}_interpolated/{frame_idx:03d}.png')\n",
    "    elif os.path.exists(f'{save_dir}/sequence{seq_idx}_{seq_idx+1}'):\n",
    "        print(f'{save_dir}/sequence{seq_idx}_{seq_idx+1}')\n",
    "        for frame_idx in range(8):\n",
    "            images.append(f'{save_dir}/sequence{seq_idx}_{seq_idx+1}/{frame_idx:03d}.png')\n",
    "    else:\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e54e31ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:12:37.932901400Z",
     "start_time": "2025-04-06T15:12:37.900716600Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4edfa016",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:12:38.803889800Z",
     "start_time": "2025-04-06T15:12:38.733160500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "120"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "300982ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:12:44.724039Z",
     "start_time": "2025-04-06T15:12:42.575647700Z"
    }
   },
   "outputs": [],
   "source": [
    "all_frames_dir = f'{save_dir}/all_frames'\n",
    "os.makedirs(all_frames_dir, exist_ok=True)\n",
    "\n",
    "for idx, image in enumerate(images):\n",
    "    shutil.copy(image, f'{all_frames_dir}/{idx:03d}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d781370f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:44:12.110158800Z",
     "start_time": "2025-04-06T15:12:46.960094400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 0\n",
      "i= 1\n",
      "i= 2\n",
      "i= 3\n",
      "i= 4\n",
      "i= 5\n",
      "i= 6\n",
      "i= 7\n",
      "i= 8\n",
      "i= 9\n",
      "i= 10\n",
      "i= 11\n",
      "i= 12\n",
      "i= 13\n",
      "i= 14\n",
      "i= 15\n",
      "i= 16\n",
      "i= 17\n",
      "i= 18\n",
      "i= 19\n",
      "i= 20\n",
      "i= 21\n",
      "i= 22\n",
      "i= 23\n",
      "i= 24\n",
      "i= 25\n",
      "i= 26\n",
      "i= 27\n",
      "i= 28\n",
      "i= 29\n",
      "i= 30\n",
      "i= 31\n",
      "i= 32\n",
      "i= 33\n",
      "i= 34\n",
      "i= 35\n",
      "i= 36\n",
      "i= 37\n",
      "i= 38\n",
      "i= 39\n",
      "i= 40\n",
      "i= 41\n",
      "i= 42\n",
      "i= 43\n",
      "i= 44\n",
      "i= 45\n",
      "i= 46\n",
      "i= 47\n",
      "i= 48\n",
      "i= 49\n",
      "i= 50\n",
      "i= 51\n",
      "i= 52\n",
      "i= 53\n",
      "i= 54\n",
      "i= 55\n",
      "i= 56\n",
      "i= 57\n",
      "i= 58\n",
      "i= 59\n",
      "i= 60\n",
      "i= 61\n",
      "i= 62\n",
      "i= 63\n",
      "i= 64\n",
      "i= 65\n",
      "i= 66\n",
      "i= 67\n",
      "i= 68\n",
      "i= 69\n",
      "i= 70\n",
      "i= 71\n",
      "i= 72\n",
      "i= 73\n",
      "i= 74\n",
      "i= 75\n",
      "i= 76\n",
      "i= 77\n",
      "i= 78\n",
      "i= 79\n",
      "i= 80\n",
      "i= 81\n",
      "i= 82\n",
      "i= 83\n",
      "i= 84\n",
      "i= 85\n",
      "i= 86\n",
      "i= 87\n",
      "i= 88\n",
      "i= 89\n",
      "i= 90\n",
      "i= 91\n",
      "i= 92\n",
      "i= 93\n",
      "i= 94\n",
      "i= 95\n",
      "i= 96\n",
      "i= 97\n",
      "i= 98\n",
      "i= 99\n",
      "i= 100\n",
      "i= 101\n",
      "i= 102\n",
      "i= 103\n",
      "i= 104\n",
      "i= 105\n",
      "i= 106\n",
      "i= 107\n",
      "i= 108\n",
      "i= 109\n",
      "i= 110\n",
      "i= 111\n",
      "i= 112\n",
      "i= 113\n",
      "i= 114\n",
      "i= 115\n",
      "i= 116\n",
      "i= 117\n",
      "i= 118\n",
      "i= 119\n"
     ]
    }
   ],
   "source": [
    "target_dir = f'{save_dir}/all_frames_stabilized'\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "motion_model.video_stabilization(x_identity, all_frames_dir, target_dir, fix_video_len=len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5313d931",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:44:17.624024700Z",
     "start_time": "2025-04-06T15:44:15.211711300Z"
    }
   },
   "outputs": [],
   "source": [
    "video_file_name = f'{save_dir}/video.mp4'\n",
    "\n",
    "images = []\n",
    "for i in range(1000):\n",
    "    images.append(f'{target_dir}/{i:03d}.png')\n",
    "\n",
    "frame = cv2.imread(images[0])\n",
    "height, width, layers = frame.shape\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "video = cv2.VideoWriter(video_file_name, fourcc, 48, (width, height))\n",
    "\n",
    "for image in images:\n",
    "    video.write(cv2.imread(image))\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5017f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
